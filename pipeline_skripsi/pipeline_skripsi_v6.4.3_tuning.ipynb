{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d44431c",
   "metadata": {},
   "source": [
    "svm_model = SVC(kernel='rbf', C=10.0, gamma='scale', probability=True)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb451e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Semua pustaka berhasil diimpor.\n",
      "Direktori telah disiapkan.\n",
      "Menggunakan device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============== LANGKAH 1: INISIALISASI & SETUP =============\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from deepface import DeepFace # Diperlukan untuk get_embedding gallery\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Semua pustaka berhasil diimpor.\")\n",
    "\n",
    "# Definisikan kelas Encoder kustom (jika diperlukan untuk menyimpan hasil baru)\n",
    "class NumpyJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\" Custom encoder for numpy data types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        elif isinstance(obj, np.floating): return float(obj)\n",
    "        elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_): return bool(obj)\n",
    "        else: return super(NumpyJSONEncoder, self).default(obj)\n",
    "\n",
    "# --- Konfigurasi Path ---\n",
    "BASE_DIR = os.path.abspath('.')\n",
    "GALLERY_PATH = os.path.join(BASE_DIR, 'data', 'gallery6.2') # Path galeri tetap diperlukan\n",
    "RESULTS_PATH = os.path.join(BASE_DIR, 'results_v6.4.3_recognition') # Path output analisis\n",
    "CACHE_PATH = os.path.join(BASE_DIR, 'cache_v6.4.3_recognition') # Path cache jika diperlukan\n",
    "# >>>>> PERUBAHAN PATH MODEL BARU <<<<<\n",
    "NEW_MODELS_PATH = os.path.join(BASE_DIR, 'models_v6.4.3_tuned') # Path BARU untuk menyimpan model hasil tuning\n",
    "PROBE_FEATURES_PATH = os.path.join(BASE_DIR, 'features_v6.4') # Path fitur probe dari v6.4\n",
    "\n",
    "# Pastikan semua direktori ada\n",
    "for path in [RESULTS_PATH, CACHE_PATH, NEW_MODELS_PATH]: # Tambahkan NEW_MODELS_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "print(\"Direktori telah disiapkan.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Menggunakan device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea45974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi-fungsi utilitas siap digunakan.\n"
     ]
    }
   ],
   "source": [
    "# ============== LANGKAH 2: DEFINISI FUNGSI UTILITAS =============\n",
    "\n",
    "# Fungsi get_embedding diperlukan untuk melatih classifier dari galeri\n",
    "def get_embedding(image_path_or_array, model_name='ArcFace', detector_backend='retinaface') -> list | None:\n",
    "    \"\"\"Mengekstrak embedding wajah dari path gambar atau array (dengan deteksi).\"\"\"\n",
    "    try:\n",
    "        embedding_objs = DeepFace.represent(\n",
    "            img_path=image_path_or_array, model_name=model_name,\n",
    "            enforce_detection=True, detector_backend=detector_backend\n",
    "        )\n",
    "        if embedding_objs and isinstance(embedding_objs, list):\n",
    "            return embedding_objs[0]['embedding']\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error saat get_embedding: {e}\") # Uncomment untuk debug\n",
    "        return None\n",
    "\n",
    "# Fungsi Cosine Similarity diperlukan untuk perbandingan\n",
    "def cosine_similarity_prediction(query_embedding, gallery_embeddings, gallery_labels, threshold=0.5):\n",
    "    \"\"\"Prediksi menggunakan cosine similarity.\"\"\"\n",
    "    if query_embedding is None or not gallery_embeddings:\n",
    "        return \"unknown\", 0.0, False\n",
    "\n",
    "    query_embedding = np.array(query_embedding)\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    if query_norm == 0:\n",
    "        return \"unknown\", 0.0, False\n",
    "    query_embedding = query_embedding / query_norm\n",
    "\n",
    "    similarities = []\n",
    "    valid_gallery_labels = []\n",
    "    for gallery_emb, label in zip(gallery_embeddings, gallery_labels):\n",
    "        gallery_emb = np.array(gallery_emb)\n",
    "        gallery_norm = np.linalg.norm(gallery_emb)\n",
    "        if gallery_norm > 0:\n",
    "            gallery_emb = gallery_emb / gallery_norm\n",
    "            similarity = np.dot(query_embedding, gallery_emb)\n",
    "            similarities.append(similarity)\n",
    "            valid_gallery_labels.append(label)\n",
    "\n",
    "    if not similarities:\n",
    "        return \"unknown\", 0.0, False\n",
    "\n",
    "    max_similarity = np.max(similarities)\n",
    "    max_idx = np.argmax(similarities)\n",
    "    predicted_label = valid_gallery_labels[max_idx]\n",
    "    is_recognized = max_similarity > threshold\n",
    "\n",
    "    if np.isnan(max_similarity):\n",
    "         return \"unknown\", 0.0, False\n",
    "\n",
    "    return predicted_label, float(max_similarity), bool(is_recognized)\n",
    "\n",
    "def cosine_similarity_top_n(query_embedding, gallery_embeddings, gallery_labels, top_n=5):\n",
    "    \"\"\"\n",
    "    Mengembalikan Top-N label beserta skor similarity-nya.\n",
    "    \"\"\"\n",
    "    if query_embedding is None or not gallery_embeddings:\n",
    "        return [], False\n",
    "\n",
    "    query_embedding = np.array(query_embedding)\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    if query_norm == 0:\n",
    "        return [], False\n",
    "    query_embedding = query_embedding / query_norm\n",
    "\n",
    "    # Hitung similarity untuk semua kandidat di galeri\n",
    "    scores = []\n",
    "    for gallery_emb, label in zip(gallery_embeddings, gallery_labels):\n",
    "        gallery_emb = np.array(gallery_emb)\n",
    "        gallery_norm = np.linalg.norm(gallery_emb)\n",
    "        if gallery_norm > 0:\n",
    "            gallery_emb = gallery_emb / gallery_norm\n",
    "            sim = np.dot(query_embedding, gallery_emb)\n",
    "            scores.append((label, sim))\n",
    "\n",
    "    # Urutkan dari similarity terbesar ke terkecil\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ambil Top-N\n",
    "    # Catatan: Ini logic \"Nearest Neighbor\". Jika ada banyak foto 'subject_a' di galeri,\n",
    "    # Top-5 bisa jadi ['a', 'a', 'a', 'b', 'c'].\n",
    "    # Jika ground_truth 'a' ada di list ini, maka hitungannya benar.\n",
    "    top_results = scores[:top_n]\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "\n",
    "print(\"Fungsi-fungsi utilitas siap digunakan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mempersiapkan data latih dari galeri...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dc03f933a34b0cbfeed50e36c3894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Membangun Dataset Latih:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset latih siap dengan 55 sampel.\n",
      "Subjek yang ditemukan: ['a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k']\n",
      "\n",
      "Melatih model K-Nearest Neighbors (KNN)...\n",
      "Model KNN selesai dilatih.\n",
      "Melatih model Support Vector Machine (SVM)...\n",
      "Model SVM selesai dilatih.\n",
      "Model KNN, SVM, dan LabelEncoder baru berhasil disimpan di: d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\models_v6.4.3_tuned\n",
      "\n",
      "Mempersiapkan gallery embeddings untuk cosine similarity...\n",
      "Cache gallery embeddings dimuat (55 sampel).\n",
      "Gallery embeddings siap dengan 55 sampel.\n"
     ]
    }
   ],
   "source": [
    "# ============== LANGKAH 3: LATIH CLASSIFIER & PERSIAPAN GALERI =============\n",
    "\n",
    "# --- Selalu Latih Classifier dari Galeri ---\n",
    "print(\"Mempersiapkan data latih dari galeri...\")\n",
    "X_train = []\n",
    "y_train_labels = []\n",
    "gallery_files = glob.glob(os.path.join(GALLERY_PATH, '*.jpg')) # Sesuaikan ekstensi\n",
    "\n",
    "if not gallery_files:\n",
    "     print(f\"ERROR: Tidak ada file ditemukan di {GALLERY_PATH}. Pelatihan dibatalkan.\")\n",
    "     sys.exit()\n",
    "\n",
    "for g_file in tqdm(gallery_files, desc=\"Membangun Dataset Latih\"):\n",
    "    subject_id = os.path.basename(g_file).split('_')[0]\n",
    "    embedding = get_embedding(g_file) # Gunakan fungsi get_embedding\n",
    "    if embedding is not None:\n",
    "        X_train.append(embedding)\n",
    "        y_train_labels.append(subject_id)\n",
    "\n",
    "if not X_train:\n",
    "     print(\"ERROR: Tidak ada embedding yang berhasil diekstrak dari galeri. Pelatihan dibatalkan.\")\n",
    "     sys.exit()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_labels)\n",
    "labels = le.classes_\n",
    "print(f\"\\nDataset latih siap dengan {len(X_train)} sampel.\")\n",
    "print(f\"Subjek yang ditemukan: {labels}\")\n",
    "\n",
    "# Latih KNN\n",
    "print(\"\\nMelatih model K-Nearest Neighbors (KNN)...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=1, weights='distance', metric='euclidean')\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"Model KNN selesai dilatih.\")\n",
    "\n",
    "# Latih SVM\n",
    "print(\"Melatih model Support Vector Machine (SVM)...\")\n",
    "svm_model = SVC(kernel='linear', probability=True, C=1000.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Model SVM selesai dilatih.\")\n",
    "\n",
    "# >>>>> PERUBAHAN PATH PENYIMPANAN <<<<<\n",
    "knn_model_path_new = os.path.join(NEW_MODELS_PATH, 'knn_model_tuned.pkl')\n",
    "svm_model_path_new = os.path.join(NEW_MODELS_PATH, 'svm_model_tuned.pkl')\n",
    "le_path_new = os.path.join(NEW_MODELS_PATH, 'label_encoder.pkl')\n",
    "\n",
    "with open(knn_model_path_new, 'wb') as f: pickle.dump(knn_model, f)\n",
    "with open(svm_model_path_new, 'wb') as f: pickle.dump(svm_model, f)\n",
    "with open(le_path_new, 'wb') as f: pickle.dump(le, f)\n",
    "print(f\"Model KNN, SVM, dan LabelEncoder baru berhasil disimpan di: {NEW_MODELS_PATH}\")\n",
    "\n",
    "\n",
    "# ============== PERSIAPAN GALLERY EMBEDDINGS UNTUK COSINE SIMILARITY =============\n",
    "print(\"\\nMempersiapkan gallery embeddings untuk cosine similarity...\")\n",
    "gallery_embeddings = []\n",
    "gallery_labels = []\n",
    "gallery_files_cosine = glob.glob(os.path.join(GALLERY_PATH, '*.jpg')) # Sesuaikan ekstensi\n",
    "\n",
    "if not gallery_files_cosine:\n",
    "     print(f\"ERROR: Tidak ada file ditemukan di {GALLERY_PATH} untuk cosine similarity.\")\n",
    "     sys.exit()\n",
    "\n",
    "# Coba muat cache embedding galeri jika ada untuk mempercepat\n",
    "gallery_cache_path = os.path.join(CACHE_PATH, 'gallery_embeddings_cache.pkl')\n",
    "try:\n",
    "    with open(gallery_cache_path, 'rb') as f:\n",
    "        gallery_embeddings, gallery_labels = pickle.load(f)\n",
    "    print(f\"Cache gallery embeddings dimuat ({len(gallery_embeddings)} sampel).\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Cache gallery embeddings tidak ditemukan, membuat ulang...\")\n",
    "    for g_file in tqdm(gallery_files_cosine, desc=\"Membangun Gallery Embeddings\"):\n",
    "        subject_id = os.path.basename(g_file).split('_')[0]\n",
    "        embedding = get_embedding(g_file)\n",
    "        if embedding is not None:\n",
    "            gallery_embeddings.append(embedding)\n",
    "            gallery_labels.append(subject_id)\n",
    "    # Simpan ke cache\n",
    "    try:\n",
    "        with open(gallery_cache_path, 'wb') as f:\n",
    "            pickle.dump((gallery_embeddings, gallery_labels), f)\n",
    "        print(\"Cache gallery embeddings disimpan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal menyimpan cache gallery embeddings: {e}\")\n",
    "\n",
    "if not gallery_embeddings:\n",
    "     print(\"ERROR: Tidak ada embedding galeri yang berhasil dibuat untuk cosine similarity.\")\n",
    "     sys.exit()\n",
    "else:\n",
    "     print(f\"Gallery embeddings siap dengan {len(gallery_embeddings)} sampel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5bf3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Berhasil memuat 1364 fitur probe dari d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\features_v6.4\\probe_features_v6.4.json\n",
      "\n",
      "=== DEBUG: Memeriksa struktur data pertama ===\n",
      "\n",
      "Tipe embedding_original: <class 'list'>\n",
      "Panjang embedding_original: 512\n",
      "\n",
      "Tipe embedding_restored: <class 'list'>\n",
      "Panjang embedding_restored: 512\n",
      "==================================================\n",
      "\n",
      "Memulai proses rekognisi untuk ORIGINAL dan RESTORED...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ae9c661a2e4838b134aa92b37346ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Melakukan Rekognisi:   0%|          | 0/1364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                    STATISTIK EMBEDDING                     \n",
      "============================================================\n",
      "Total data probe               : 1364\n",
      "Restorasi berhasil             : 1364 (100.0%)\n",
      "\n",
      "Embedding ORIGINAL valid       : 1351 (99.0%)\n",
      "Embedding RESTORED valid       : 1364 (100.0%)\n",
      "Kedua embedding valid          : 1351 (99.0%)\n",
      "============================================================\n",
      "\n",
      "Hasil rekognisi berhasil disimpan ke: d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\results_v6.4.3_recognition\\recognition_results_v6.4.3_comparison.json\n"
     ]
    }
   ],
   "source": [
    "# ============== LANGKAH 4 (REVISI LENGKAP & FIXED): MUAT FITUR PROBE & LAKUKAN REKOGNISI =============\n",
    "# Versi ini menganalisis KEDUA embedding: original dan restored\n",
    "\n",
    "features_file_path = os.path.join(PROBE_FEATURES_PATH, 'probe_features_v6.4.json')\n",
    "try:\n",
    "    with open(features_file_path, 'r', encoding='utf-8') as f:\n",
    "        probe_features = json.load(f)\n",
    "    print(f\"\\nBerhasil memuat {len(probe_features)} fitur probe dari {features_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File fitur probe tidak ditemukan di {features_file_path}.\")\n",
    "    probe_features = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"ERROR: Gagal mem-parsing file JSON fitur probe: {features_file_path}.\")\n",
    "    probe_features = []\n",
    "\n",
    "# >>>>> DEBUG: Memeriksa struktur data pertama <<<<<\n",
    "if probe_features:\n",
    "    print(\"\\n=== DEBUG: Memeriksa struktur data pertama ===\")\n",
    "    first_entry = probe_features[0]\n",
    "    # print(f\"Keys dalam entry pertama: {first_entry.keys()}\")\n",
    "    \n",
    "    emb_orig = first_entry.get('embedding_original')\n",
    "    emb_rest = first_entry.get('embedding_restored')\n",
    "    \n",
    "    print(f\"\\nTipe embedding_original: {type(emb_orig)}\")\n",
    "    if emb_orig and isinstance(emb_orig, list):\n",
    "        print(f\"Panjang embedding_original: {len(emb_orig)}\")\n",
    "    \n",
    "    print(f\"\\nTipe embedding_restored: {type(emb_rest)}\")\n",
    "    if emb_rest and isinstance(emb_rest, list):\n",
    "        print(f\"Panjang embedding_restored: {len(emb_rest)}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "recognition_results = []\n",
    "\n",
    "# Counters untuk statistik\n",
    "stats = {\n",
    "    'total': 0,\n",
    "    'original_valid': 0,\n",
    "    'restored_valid': 0,\n",
    "    'both_valid': 0,\n",
    "    'restoration_succeeded': 0\n",
    "}\n",
    "\n",
    "if not probe_features:\n",
    "    print(\"Tidak ada fitur probe untuk diproses.\")\n",
    "else:\n",
    "    print(\"\\nMemulai proses rekognisi untuk ORIGINAL dan RESTORED...\")\n",
    "    \n",
    "    for idx, feature_entry in enumerate(tqdm(probe_features, desc=\"Melakukan Rekognisi\")):\n",
    "        stats['total'] += 1\n",
    "        \n",
    "        # >>>>> AMBIL KEDUA EMBEDDING <<<<<\n",
    "        probe_embedding_original = feature_entry.get('embedding_original')\n",
    "        probe_embedding_restored = feature_entry.get('embedding_restored')\n",
    "        \n",
    "        ground_truth = feature_entry.get('ground_truth', 'unknown')\n",
    "        filename = feature_entry.get('file', 'unknown')\n",
    "        metadata = feature_entry.get('metadata', {})\n",
    "        restoration_succeeded = feature_entry.get('restoration_succeeded', False)\n",
    "        \n",
    "        if restoration_succeeded:\n",
    "            stats['restoration_succeeded'] += 1\n",
    "        \n",
    "        # >>>>> STRUKTUR RESULT ENTRY <<<<<\n",
    "        result_entry = {\n",
    "            'file': filename,\n",
    "            'ground_truth': ground_truth,\n",
    "            'metadata': metadata,\n",
    "            'restoration_succeeded': restoration_succeeded,\n",
    "            \n",
    "            # Default values (akan diupdate fungsi helper)\n",
    "            'embedding_original_found': False,\n",
    "            'embedding_restored_found': False\n",
    "        }\n",
    "        \n",
    "        # >>>>> FUNGSI HELPER UNTUK PREDIKSI (TOP-N) <<<<<\n",
    "        def predict_all_models(embedding, suffix=''):\n",
    "            results = {}\n",
    "            \n",
    "            # --- Validasi Embedding ---\n",
    "            has_valid = False\n",
    "            if isinstance(embedding, list) and len(embedding) > 0: has_valid = True\n",
    "            elif isinstance(embedding, np.ndarray) and embedding.size > 0: has_valid = True\n",
    "            \n",
    "            if not has_valid:\n",
    "                for k in range(1, 6):\n",
    "                    results[f'knn_top{k}{suffix}'] = False\n",
    "                    results[f'svm_top{k}{suffix}'] = False\n",
    "                    results[f'cosine_top{k}{suffix}'] = False\n",
    "                results[f'embedding{suffix}_found'] = False\n",
    "                # Tetap isi prediction single value agar tidak error saat akses key nanti\n",
    "                results[f'prediction_knn{suffix}'] = 'unknown'\n",
    "                results[f'prediction_svm{suffix}'] = 'unknown'\n",
    "                results[f'prediction_cosine{suffix}'] = 'unknown'\n",
    "                return results\n",
    "\n",
    "            results[f'embedding{suffix}_found'] = True\n",
    "            embedding_np = np.array(embedding).reshape(1, -1)\n",
    "            \n",
    "            # 1. PREDIKSI KNN\n",
    "            try:\n",
    "                knn_probs = knn_model.predict_proba(embedding_np)[0]\n",
    "                top_k_indices = np.argsort(knn_probs)[::-1][:5]\n",
    "                top_k_labels = le.inverse_transform(top_k_indices)\n",
    "                for k in range(1, 6):\n",
    "                    is_correct = ground_truth in top_k_labels[:k]\n",
    "                    results[f'knn_top{k}{suffix}'] = is_correct\n",
    "                results[f'prediction_knn{suffix}'] = top_k_labels[0]\n",
    "            except Exception:\n",
    "                for k in range(1, 6): results[f'knn_top{k}{suffix}'] = False\n",
    "                results[f'prediction_knn{suffix}'] = 'unknown'\n",
    "\n",
    "            # 2. PREDIKSI SVM\n",
    "            try:\n",
    "                svm_probs = svm_model.predict_proba(embedding_np)[0]\n",
    "                top_k_indices = np.argsort(svm_probs)[::-1][:5]\n",
    "                top_k_labels = le.inverse_transform(top_k_indices)\n",
    "                for k in range(1, 6):\n",
    "                    is_correct = ground_truth in top_k_labels[:k]\n",
    "                    results[f'svm_top{k}{suffix}'] = is_correct\n",
    "                results[f'prediction_svm{suffix}'] = top_k_labels[0]\n",
    "            except Exception:\n",
    "                for k in range(1, 6): results[f'svm_top{k}{suffix}'] = False\n",
    "                results[f'prediction_svm{suffix}'] = 'unknown'\n",
    "\n",
    "            # 3. PREDIKSI COSINE\n",
    "            try:\n",
    "                top_n_results = cosine_similarity_top_n(embedding, gallery_embeddings, gallery_labels, top_n=5)\n",
    "                top_k_labels = [label for label, score in top_n_results]\n",
    "                for k in range(1, 6):\n",
    "                    if len(top_k_labels) >= k: is_correct = ground_truth in top_k_labels[:k]\n",
    "                    else: is_correct = ground_truth in top_k_labels\n",
    "                    results[f'cosine_top{k}{suffix}'] = is_correct\n",
    "\n",
    "                if top_n_results:\n",
    "                    results[f'prediction_cosine{suffix}'] = top_n_results[0][0]\n",
    "                    results[f'similarity_cosine{suffix}'] = top_n_results[0][1]\n",
    "                else:\n",
    "                    results[f'prediction_cosine{suffix}'] = 'unknown'\n",
    "            except Exception:\n",
    "                for k in range(1, 6): results[f'cosine_top{k}{suffix}'] = False\n",
    "                results[f'prediction_cosine{suffix}'] = 'unknown'\n",
    "\n",
    "            return results\n",
    "        \n",
    "        # ======================================================================\n",
    "        # >>>>> BAGIAN YANG HILANG (EKSEKUSI FUNGSI) <<<<<\n",
    "        # ======================================================================\n",
    "        \n",
    "        # 1. Proses Original\n",
    "        original_results = predict_all_models(probe_embedding_original, '_original')\n",
    "        result_entry.update(original_results)\n",
    "        if original_results.get('embedding_original_found'):\n",
    "            stats['original_valid'] += 1\n",
    "            \n",
    "        # 2. Proses Restored\n",
    "        restored_results = predict_all_models(probe_embedding_restored, '_restored')\n",
    "        result_entry.update(restored_results)\n",
    "        if restored_results.get('embedding_restored_found'):\n",
    "            stats['restored_valid'] += 1\n",
    "            \n",
    "        # 3. Hitung Both Valid\n",
    "        if original_results.get('embedding_original_found') and restored_results.get('embedding_restored_found'):\n",
    "            stats['both_valid'] += 1\n",
    "            \n",
    "        recognition_results.append(result_entry)\n",
    "    \n",
    "    # >>>>> TAMPILKAN STATISTIK <<<<<\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"STATISTIK EMBEDDING\".center(60))\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total data probe               : {stats['total']}\")\n",
    "    print(f\"Restorasi berhasil             : {stats['restoration_succeeded']} ({stats['restoration_succeeded']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"\\nEmbedding ORIGINAL valid       : {stats['original_valid']} ({stats['original_valid']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"Embedding RESTORED valid       : {stats['restored_valid']} ({stats['restored_valid']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"Kedua embedding valid          : {stats['both_valid']} ({stats['both_valid']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # --- Simpan Hasil Rekognisi ---\n",
    "    results_file_path = os.path.join(RESULTS_PATH, 'recognition_results_v6.4.3_comparison.json')\n",
    "    try:\n",
    "        with open(results_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(recognition_results, f, indent=4, cls=NumpyJSONEncoder)\n",
    "        print(f\"Hasil rekognisi berhasil disimpan ke: {results_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saat menyimpan hasil rekognisi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413e0073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                MEMULAI ANALISIS PERBANDINGAN ORIGINAL VS RESTORED (TOP-1 s/d TOP-5)                \n",
      "====================================================================================================\n",
      "\n",
      "Data dengan embedding ORIGINAL valid: 1351\n",
      "Data dengan embedding RESTORED valid: 1364\n",
      "\n",
      "========================================================================================================================\n",
      "                                     TABEL AKURASI TOP-N: K-Nearest Neighbors (KNN)                                     \n",
      "========================================================================================================================\n",
      "              Skenario  Kondisi  Top-1  Top-2  Top-3  Top-4  Top-5\n",
      "    Jarak Dekat (< 7m) Original 84.10% 84.33% 84.33% 87.10% 90.78%\n",
      "    Jarak Dekat (< 7m) Restored 77.27% 77.73% 78.18% 82.05% 86.14%\n",
      "Jarak Menengah (7-12m) Original 56.04% 57.18% 58.54% 64.46% 69.93%\n",
      "Jarak Menengah (7-12m) Restored 72.95% 74.09% 74.77% 79.77% 84.77%\n",
      "   Jarak Jauh (>= 12m) Original 11.30% 19.46% 28.45% 37.24% 46.03%\n",
      "   Jarak Jauh (>= 12m) Restored 41.53% 46.07% 52.27% 60.54% 66.94%\n",
      "       Ketinggian 1.5m Original 58.82% 61.76% 65.29% 70.59% 75.29%\n",
      "       Ketinggian 1.5m Restored 65.69% 67.74% 70.97% 75.95% 81.23%\n",
      "         Ketinggian 3m Original 57.23% 60.47% 64.31% 69.03% 73.16%\n",
      "         Ketinggian 3m Restored 70.38% 72.73% 75.37% 81.52% 83.87%\n",
      "         Ketinggian 4m Original 49.12% 52.35% 55.29% 60.88% 67.94%\n",
      "         Ketinggian 4m Restored 68.62% 70.67% 72.73% 76.25% 80.65%\n",
      "         Ketinggian 5m Original 31.33% 35.24% 39.46% 47.59% 56.02%\n",
      "         Ketinggian 5m Restored 48.09% 50.15% 52.49% 61.00% 69.79%\n",
      "        Semua Data Uji Original 49.22% 52.55% 56.18% 62.10% 68.17%\n",
      "        Semua Data Uji Restored 63.20% 65.32% 67.89% 73.68% 78.89%\n",
      "========================================================================================================================\n",
      "\n",
      "Tabel disimpan ke: d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\results_v6.4.3_recognition\\top_n_accuracy_knn.csv\n",
      "\n",
      "========================================================================================================================\n",
      "                                   TABEL AKURASI TOP-N: Support Vector Machine (SVM)                                    \n",
      "========================================================================================================================\n",
      "              Skenario  Kondisi  Top-1  Top-2  Top-3  Top-4  Top-5\n",
      "    Jarak Dekat (< 7m) Original 83.64% 89.17% 94.01% 95.39% 96.54%\n",
      "    Jarak Dekat (< 7m) Restored 75.68% 82.95% 84.55% 86.59% 86.82%\n",
      "Jarak Menengah (7-12m) Original 53.76% 66.51% 77.68% 81.09% 86.10%\n",
      "Jarak Menengah (7-12m) Restored 70.00% 79.55% 84.09% 87.05% 88.41%\n",
      "   Jarak Jauh (>= 12m) Original 12.76% 21.76% 33.26% 44.98% 51.88%\n",
      "   Jarak Jauh (>= 12m) Restored 36.36% 53.93% 63.84% 74.38% 82.02%\n",
      "       Ketinggian 1.5m Original 58.82% 66.18% 73.24% 77.06% 81.18%\n",
      "       Ketinggian 1.5m Restored 61.00% 72.43% 78.59% 84.46% 86.51%\n",
      "         Ketinggian 3m Original 57.52% 64.90% 73.16% 79.35% 82.89%\n",
      "         Ketinggian 3m Restored 67.74% 80.94% 84.75% 88.56% 92.67%\n",
      "         Ketinggian 4m Original 50.88% 60.29% 66.47% 72.65% 76.47%\n",
      "         Ketinggian 4m Restored 63.93% 74.49% 80.06% 84.75% 89.15%\n",
      "         Ketinggian 5m Original 27.71% 40.06% 55.72% 62.35% 68.67%\n",
      "         Ketinggian 5m Restored 46.92% 58.36% 64.81% 71.85% 74.19%\n",
      "        Semua Data Uji Original 48.85% 57.96% 67.21% 72.91% 77.35%\n",
      "        Semua Data Uji Restored 59.90% 71.55% 77.05% 82.40% 85.63%\n",
      "========================================================================================================================\n",
      "\n",
      "Tabel disimpan ke: d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\results_v6.4.3_recognition\\top_n_accuracy_svm.csv\n",
      "\n",
      "========================================================================================================================\n",
      "                                         TABEL AKURASI TOP-N: Cosine Similarity                                         \n",
      "========================================================================================================================\n",
      "              Skenario  Kondisi  Top-1  Top-2  Top-3  Top-4  Top-5\n",
      "    Jarak Dekat (< 7m) Original 89.40% 91.24% 93.78% 95.85% 96.31%\n",
      "    Jarak Dekat (< 7m) Restored 78.64% 78.64% 82.05% 85.23% 86.59%\n",
      "Jarak Menengah (7-12m) Original 65.15% 70.62% 75.17% 80.87% 84.28%\n",
      "Jarak Menengah (7-12m) Restored 76.59% 78.41% 81.14% 83.18% 85.23%\n",
      "   Jarak Jauh (>= 12m) Original 15.06% 17.36% 25.52% 40.38% 46.86%\n",
      "   Jarak Jauh (>= 12m) Restored 45.66% 52.48% 58.06% 63.64% 67.98%\n",
      "       Ketinggian 1.5m Original 62.35% 65.88% 69.12% 76.47% 81.47%\n",
      "       Ketinggian 1.5m Restored 69.21% 72.43% 75.37% 78.30% 80.65%\n",
      "         Ketinggian 3m Original 63.72% 66.37% 68.44% 76.40% 79.65%\n",
      "         Ketinggian 3m Restored 77.13% 79.77% 84.16% 85.63% 87.10%\n",
      "         Ketinggian 4m Original 54.71% 58.53% 65.59% 72.06% 75.29%\n",
      "         Ketinggian 4m Restored 72.14% 73.90% 77.42% 80.94% 84.16%\n",
      "         Ketinggian 5m Original 39.76% 42.47% 50.90% 60.24% 62.95%\n",
      "         Ketinggian 5m Restored 46.63% 51.03% 56.01% 62.76% 66.28%\n",
      "        Semua Data Uji Original 55.22% 58.40% 63.58% 71.35% 74.91%\n",
      "        Semua Data Uji Restored 66.28% 69.28% 73.24% 76.91% 79.55%\n",
      "========================================================================================================================\n",
      "\n",
      "Tabel disimpan ke: d:\\UNSRI_DATA\\_SKRIPSI\\PROGRAM\\v1\\pipeline_skripsi\\results_v6.4.3_recognition\\top_n_accuracy_cosine.csv\n",
      "\n",
      "================================================================================\n",
      "                                ANALISIS SELESAI                                \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if not recognition_results:\n",
    "    print(\"\\nTidak ada hasil rekognisi untuk dianalisis.\")\n",
    "else:\n",
    "    df_results = pd.DataFrame(recognition_results)\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"MEMULAI ANALISIS PERBANDINGAN ORIGINAL VS RESTORED (TOP-1 s/d TOP-5)\".center(100))\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # >>>>> FILTER DATASET <<<<<\n",
    "    df_original = df_results[df_results['embedding_original_found'] == True].copy()\n",
    "    df_restored = df_results[df_results['embedding_restored_found'] == True].copy()\n",
    "    \n",
    "    if df_original.empty and df_restored.empty:\n",
    "        print(\"Tidak ada hasil rekognisi yang memiliki embedding untuk dianalisis.\")\n",
    "    else:\n",
    "        # Prepare metadata columns\n",
    "        for df in [df_original, df_restored]:\n",
    "            if not df.empty:\n",
    "                df['distance_m'] = df['metadata'].apply(lambda x: x.get('distance_m') if isinstance(x, dict) else None)\n",
    "                df['height_m'] = df['metadata'].apply(lambda x: x.get('height_m') if isinstance(x, dict) else None)\n",
    "        \n",
    "        print(f\"Data dengan embedding ORIGINAL valid: {len(df_original)}\")\n",
    "        print(f\"Data dengan embedding RESTORED valid: {len(df_restored)}\")\n",
    "\n",
    "        # >>>>> DEFINISI SKENARIO <<<<<\n",
    "        scenarios = [\n",
    "            {\"name\": \"Jarak Dekat (< 7m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: x < 7},\n",
    "            {\"name\": \"Jarak Menengah (7-12m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: 7 <= x < 12},\n",
    "            {\"name\": \"Jarak Jauh (>= 12m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: x >= 12},\n",
    "            {\"name\": \"Ketinggian 1.5m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 1.5},\n",
    "            {\"name\": \"Ketinggian 3m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 3},\n",
    "            {\"name\": \"Ketinggian 4m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 4},\n",
    "            {\"name\": \"Ketinggian 5m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 5},\n",
    "            {\"name\": \"Semua Data Uji\", \"filter_key\": None, \"condition\": None}\n",
    "        ]\n",
    "        \n",
    "        models = ['knn', 'svm', 'cosine']\n",
    "        model_names = {\n",
    "            'knn': 'K-Nearest Neighbors (KNN)',\n",
    "            'svm': 'Support Vector Machine (SVM)',\n",
    "            'cosine': 'Cosine Similarity'\n",
    "        }\n",
    "\n",
    "        # >>>>> FUNGSI HITUNG TOP-N <<<<<\n",
    "        def get_top_n_accuracy(df, model, suffix, n=5):\n",
    "            \"\"\"Menghasilkan list akurasi [Top-1, Top-2, ..., Top-N]\"\"\"\n",
    "            accuracies = []\n",
    "            for k in range(1, n + 1):\n",
    "                # Nama kolom harus sesuai dengan output Langkah 4 (misal: knn_top1_original)\n",
    "                col_name = f'{model}_top{k}{suffix}'\n",
    "                \n",
    "                # Fallback jika kolom top-n tidak ditemukan (misal: pakai is_correct biasa untuk top-1)\n",
    "                fallback_col = f'is_correct_{model}{suffix}'\n",
    "                \n",
    "                if col_name in df.columns:\n",
    "                    acc = df[col_name].mean() * 100\n",
    "                    accuracies.append(f\"{acc:.2f}%\")\n",
    "                elif k == 1 and fallback_col in df.columns:\n",
    "                    acc = df[fallback_col].mean() * 100\n",
    "                    accuracies.append(f\"{acc:.2f}%\")\n",
    "                else:\n",
    "                    accuracies.append(\"N/A\")\n",
    "            return accuracies\n",
    "\n",
    "        # >>>>> GENERATE TABEL TOP-N <<<<<\n",
    "        for model in models:\n",
    "            table_data = []\n",
    "            \n",
    "            for scenario in scenarios:\n",
    "                scenario_name = scenario[\"name\"]\n",
    "                \n",
    "                # Filter Data\n",
    "                if scenario[\"filter_key\"] is None:\n",
    "                    filt_orig = df_original\n",
    "                    filt_rest = df_restored\n",
    "                else:\n",
    "                    key = scenario[\"filter_key\"]\n",
    "                    cond = scenario[\"condition\"]\n",
    "                    filt_orig = df_original[df_original[key].apply(cond)] if not df_original.empty else pd.DataFrame()\n",
    "                    filt_rest = df_restored[df_restored[key].apply(cond)] if not df_restored.empty else pd.DataFrame()\n",
    "                \n",
    "                # --- Row untuk Original ---\n",
    "                if not filt_orig.empty:\n",
    "                    accs_orig = get_top_n_accuracy(filt_orig, model, '_original')\n",
    "                    row_orig = [scenario_name, 'Original'] + accs_orig\n",
    "                    table_data.append(row_orig)\n",
    "                else:\n",
    "                    table_data.append([scenario_name, 'Original'] + ['N/A']*5)\n",
    "                \n",
    "                # --- Row untuk Restored ---\n",
    "                if not filt_rest.empty:\n",
    "                    accs_rest = get_top_n_accuracy(filt_rest, model, '_restored')\n",
    "                    row_rest = [scenario_name, 'Restored'] + accs_rest\n",
    "                    table_data.append(row_rest)\n",
    "                else:\n",
    "                    table_data.append([scenario_name, 'Restored'] + ['N/A']*5)\n",
    "            \n",
    "            # --- Buat DataFrame & Tampilkan ---\n",
    "            columns_top_n = ['Skenario', 'Kondisi', 'Top-1', 'Top-2', 'Top-3', 'Top-4', 'Top-5']\n",
    "            df_table = pd.DataFrame(table_data, columns=columns_top_n)\n",
    "            \n",
    "            print(f\"\\n{'='*120}\")\n",
    "            print(f\"TABEL AKURASI TOP-N: {model_names[model]}\".center(120))\n",
    "            print(f\"{'='*120}\")\n",
    "            print(df_table.to_string(index=False))\n",
    "            print(f\"{'='*120}\\n\")\n",
    "            \n",
    "            # Simpan CSV\n",
    "            save_path = os.path.join(RESULTS_PATH, f'top_n_accuracy_{model}.csv')\n",
    "            df_table.to_csv(save_path, index=False)\n",
    "            print(f\"Tabel disimpan ke: {save_path}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALISIS SELESAI\".center(80))\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367660ff",
   "metadata": {},
   "source": [
    "# ============== LANGKAH 5: ANALISIS HASIL REKOGNISI (ORIGINAL VS RESTORED) =============\n",
    "\n",
    "if not recognition_results:\n",
    "\n",
    "    print(\"\\nTidak ada hasil rekognisi untuk dianalisis.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    df_results = pd.DataFrame(recognition_results)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    print(\"MEMULAI ANALISIS PERBANDINGAN ORIGINAL VS RESTORED\".center(80))\n",
    "\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # >>>>> ANALISIS UNTUK ORIGINAL <<<<<\n",
    "\n",
    "    df_original = df_results[df_results['embedding_original_found'] == True].copy()\n",
    "\n",
    "\n",
    "\n",
    "    # >>>>> ANALISIS UNTUK RESTORED <<<<<\n",
    "\n",
    "    df_restored = df_results[df_results['embedding_restored_found'] == True].copy()\n",
    "\n",
    "\n",
    "\n",
    "    if df_original.empty and df_restored.empty:\n",
    "\n",
    "        print(\"Tidak ada hasil rekognisi yang memiliki embedding untuk dianalisis.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Prepare metadata columns untuk kedua dataframe\n",
    "\n",
    "        for df in [df_original, df_restored]:\n",
    "\n",
    "            if not df.empty:\n",
    "\n",
    "                df['distance_m'] = df['metadata'].apply(lambda x: x.get('distance_m') if isinstance(x, dict) else None)\n",
    "\n",
    "                df['height_m'] = df['metadata'].apply(lambda x: x.get('height_m') if isinstance(x, dict) else None)\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Data dengan embedding ORIGINAL valid: {len(df_original)}\")\n",
    "\n",
    "        print(f\"Data dengan embedding RESTORED valid: {len(df_restored)}\")\n",
    "\n",
    "\n",
    "\n",
    "        # >>>>> FUNGSI HELPER UNTUK ANALISIS <<<<<\n",
    "\n",
    "        def print_classification_report(title, y_true, y_pred, labels_list, save_cm=True):\n",
    "\n",
    "            \"\"\"Cetak classification report dan confusion matrix\"\"\"\n",
    "\n",
    "            print(f\"\\n{'='*80}\")\n",
    "\n",
    "            print(f\"{title}\".center(80))\n",
    "\n",
    "            print(f\"{'='*80}\")\n",
    "\n",
    "            print(classification_report(y_true, y_pred, labels=labels_list, zero_division=0))\n",
    "\n",
    "\n",
    "\n",
    "            if save_cm:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    cm = confusion_matrix(y_true, y_pred, labels=labels_list)\n",
    "\n",
    "                    plt.figure(figsize=(10, 8))\n",
    "\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis',\n",
    "\n",
    "                                xticklabels=labels_list, yticklabels=labels_list)\n",
    "\n",
    "                    plt.title(title)\n",
    "\n",
    "                    plt.ylabel('Label Sebenarnya (Ground Truth)')\n",
    "\n",
    "                    plt.xlabel('Label Prediksi')\n",
    "\n",
    "                    plot_filename = f\"{title.replace(' ', '_').replace('(', '').replace(')', '').lower()}.png\"\n",
    "\n",
    "                    plot_path = os.path.join(RESULTS_PATH, plot_filename)\n",
    "\n",
    "                    plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "\n",
    "                    plt.close()\n",
    "\n",
    "                    print(f\"Confusion matrix disimpan ke: {plot_filename}\")\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f\"Gagal membuat confusion matrix: {e}\")\n",
    "\n",
    "            print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "        def calculate_metrics(df, suffix, labels_list):\n",
    "\n",
    "            \"\"\"Hitung metrik untuk satu set data\"\"\"\n",
    "\n",
    "            metrics = {}\n",
    "\n",
    "            for model in ['knn', 'svm', 'cosine']:\n",
    "\n",
    "                is_correct_col = f'is_correct_{model}{suffix}'\n",
    "\n",
    "                prediction_col = f'prediction_{model}{suffix}'\n",
    "\n",
    "\n",
    "\n",
    "                if is_correct_col in df.columns:\n",
    "\n",
    "                    y_true = df['ground_truth']\n",
    "\n",
    "                    y_pred = df[prediction_col]\n",
    "\n",
    "\n",
    "\n",
    "                    accuracy = df[is_correct_col].mean()\n",
    "\n",
    "                    recall = recall_score(y_true, y_pred, average='macro', zero_division=0, labels=labels_list)\n",
    "\n",
    "                    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0, labels=labels_list)\n",
    "\n",
    "\n",
    "\n",
    "                    metrics[model] = {\n",
    "\n",
    "                        'accuracy': accuracy,\n",
    "\n",
    "                        'recall': recall,\n",
    "\n",
    "                        'f1': f1\n",
    "\n",
    "                    }\n",
    "\n",
    "            return metrics\n",
    "\n",
    "\n",
    "\n",
    "        # >>>>> ANALISIS OVERALL (CONFUSION MATRIX & REPORT) <<<<<\n",
    "\n",
    "\n",
    "\n",
    "        # Get labels yang ada di data\n",
    "\n",
    "        labels_original = sorted(df_original['ground_truth'].unique()) if not df_original.empty else []\n",
    "\n",
    "        labels_restored = sorted(df_restored['ground_truth'].unique()) if not df_restored.empty else []\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "        print(\"BAGIAN 1: CLASSIFICATION REPORT & CONFUSION MATRIX\".center(80))\n",
    "\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "        # --- ORIGINAL ---\n",
    "\n",
    "        if not df_original.empty:\n",
    "\n",
    "            print(\"\\n>>> EMBEDDING ORIGINAL <<<\\n\")\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"KNN - Original Embedding\",\n",
    "\n",
    "                df_original['ground_truth'],\n",
    "\n",
    "                df_original['prediction_knn_original'],\n",
    "\n",
    "                labels_original\n",
    "\n",
    "            )\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"SVM - Original Embedding\",\n",
    "\n",
    "                df_original['ground_truth'],\n",
    "\n",
    "                df_original['prediction_svm_original'],\n",
    "\n",
    "                labels_original\n",
    "\n",
    "            )\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"Cosine Similarity - Original Embedding\",\n",
    "\n",
    "                df_original['ground_truth'],\n",
    "\n",
    "                df_original['prediction_cosine_original'],\n",
    "\n",
    "                labels_original\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # --- RESTORED ---\n",
    "\n",
    "        if not df_restored.empty:\n",
    "\n",
    "            print(\"\\n>>> EMBEDDING RESTORED <<<\\n\")\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"KNN - Restored Embedding\",\n",
    "\n",
    "                df_restored['ground_truth'],\n",
    "\n",
    "                df_restored['prediction_knn_restored'],\n",
    "\n",
    "                labels_restored\n",
    "\n",
    "            )\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"SVM - Restored Embedding\",\n",
    "\n",
    "                df_restored['ground_truth'],\n",
    "\n",
    "                df_restored['prediction_svm_restored'],\n",
    "\n",
    "                labels_restored\n",
    "\n",
    "            )\n",
    "\n",
    "            print_classification_report(\n",
    "\n",
    "                \"Cosine Similarity - Restored Embedding\",\n",
    "\n",
    "                df_restored['ground_truth'],\n",
    "\n",
    "                df_restored['prediction_cosine_restored'],\n",
    "\n",
    "                labels_restored\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # >>>>> BAGIAN 2: TABEL PERBANDINGAN BERDASARKAN SKENARIO <<<<<\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "        print(\"BAGIAN 2: TABEL PERBANDINGAN KINERJA BERDASARKAN SKENARIO\".center(80))\n",
    "\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        # Define scenarios\n",
    "\n",
    "        scenarios = [\n",
    "\n",
    "            {\"name\": \"Jarak Dekat (< 7m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: x < 7},\n",
    "\n",
    "            {\"name\": \"Jarak Menengah (7-12m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: 7 <= x < 12},\n",
    "\n",
    "            {\"name\": \"Jarak Jauh (>= 12m)\", \"filter_key\": \"distance_m\", \"condition\": lambda x: x >= 12},\n",
    "\n",
    "            {\"name\": \"Ketinggian 1.5m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 1.5},\n",
    "\n",
    "            {\"name\": \"Ketinggian 3m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 3},\n",
    "\n",
    "            {\"name\": \"Ketinggian 4m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 4},\n",
    "\n",
    "            {\"name\": \"Ketinggian 5m\", \"filter_key\": \"height_m\", \"condition\": lambda x: x == 5},\n",
    "\n",
    "            {\"name\": \"Semua Data Uji\", \"filter_key\": None, \"condition\": None}\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Create separate tables for each model\n",
    "\n",
    "        models = ['knn', 'svm', 'cosine']\n",
    "\n",
    "        model_names = {\n",
    "\n",
    "            'knn': 'K-Nearest Neighbors (KNN)',\n",
    "\n",
    "            'svm': 'Support Vector Machine (SVM)',\n",
    "\n",
    "            'cosine': 'Cosine Similarity'\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        all_tables = {}\n",
    "\n",
    "\n",
    "\n",
    "        for model in models:\n",
    "\n",
    "            comparison_data = []\n",
    "\n",
    "\n",
    "\n",
    "            for scenario in scenarios:\n",
    "\n",
    "                scenario_name = scenario[\"name\"]\n",
    "\n",
    "\n",
    "\n",
    "                # Filter data untuk scenario\n",
    "\n",
    "                if scenario[\"filter_key\"] is None:\n",
    "\n",
    "                    # Semua data\n",
    "\n",
    "                    filtered_orig = df_original\n",
    "\n",
    "                    filtered_rest = df_restored\n",
    "\n",
    "                else:\n",
    "\n",
    "                    filter_key = scenario[\"filter_key\"]\n",
    "\n",
    "                    condition = scenario[\"condition\"]\n",
    "\n",
    "                    filtered_orig = df_original[\n",
    "\n",
    "                        df_original[filter_key].notna() &\n",
    "\n",
    "                        df_original[filter_key].apply(condition)\n",
    "\n",
    "                    ] if not df_original.empty else pd.DataFrame()\n",
    "\n",
    "                    filtered_rest = df_restored[\n",
    "\n",
    "                        df_restored[filter_key].notna() &\n",
    "\n",
    "                        df_restored[filter_key].apply(condition)\n",
    "\n",
    "                    ] if not df_restored.empty else pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate metrics untuk ORIGINAL\n",
    "\n",
    "                if not filtered_orig.empty:\n",
    "\n",
    "                    labels_scenario = sorted(filtered_orig['ground_truth'].unique())\n",
    "\n",
    "                    metrics_orig = calculate_metrics(filtered_orig, '_original', labels_scenario)\n",
    "\n",
    "\n",
    "\n",
    "                    if model in metrics_orig:\n",
    "\n",
    "                        acc_orig = metrics_orig[model]['accuracy']\n",
    "\n",
    "                        rec_orig = metrics_orig[model]['recall']\n",
    "\n",
    "                        f1_orig = metrics_orig[model]['f1']\n",
    "\n",
    "                        comparison_data.append([\n",
    "\n",
    "                            scenario_name,\n",
    "\n",
    "                            'Tanpa Restorasi',\n",
    "\n",
    "                            f'{acc_orig:.2%}',\n",
    "\n",
    "                            f'{rec_orig:.2f}',\n",
    "\n",
    "                            f'{f1_orig:.2f}'\n",
    "\n",
    "                        ])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    comparison_data.append([\n",
    "\n",
    "                        scenario_name,\n",
    "\n",
    "                        'Tanpa Restorasi',\n",
    "\n",
    "                        'N/A',\n",
    "\n",
    "                        'N/A',\n",
    "\n",
    "                        'N/A'\n",
    "\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate metrics untuk RESTORED\n",
    "\n",
    "                if not filtered_rest.empty:\n",
    "\n",
    "                    labels_scenario = sorted(filtered_rest['ground_truth'].unique())\n",
    "\n",
    "                    metrics_rest = calculate_metrics(filtered_rest, '_restored', labels_scenario)\n",
    "\n",
    "\n",
    "\n",
    "                    if model in metrics_rest:\n",
    "\n",
    "                        acc_rest = metrics_rest[model]['accuracy']\n",
    "\n",
    "                        rec_rest = metrics_rest[model]['recall']\n",
    "\n",
    "                        f1_rest = metrics_rest[model]['f1']\n",
    "\n",
    "\n",
    "\n",
    "                        # Calculate improvement\n",
    "\n",
    "                        if not filtered_orig.empty and model in metrics_orig:\n",
    "\n",
    "                            acc_orig = metrics_orig[model]['accuracy']\n",
    "\n",
    "                            improvement = ((acc_rest - acc_orig) / acc_orig * 100) if acc_orig > 0 else 0\n",
    "\n",
    "                            improvement_str = f'{improvement:+.1f}%'\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            improvement_str = 'N/A'\n",
    "\n",
    "\n",
    "\n",
    "                        comparison_data.append([\n",
    "\n",
    "                            scenario_name,\n",
    "\n",
    "                            'Dengan Restorasi',\n",
    "\n",
    "                            f'{acc_rest:.2%}',\n",
    "\n",
    "                            f'{rec_rest:.2f}',\n",
    "\n",
    "                            f'{f1_rest:.2f}',\n",
    "\n",
    "                            improvement_str\n",
    "\n",
    "                        ])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    comparison_data.append([\n",
    "\n",
    "                        scenario_name,\n",
    "\n",
    "                        'Dengan Restorasi',\n",
    "\n",
    "                        'N/A',\n",
    "\n",
    "                        'N/A',\n",
    "\n",
    "                        'N/A',\n",
    "\n",
    "                        'N/A'\n",
    "\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n",
    "            all_tables[model] = comparison_data\n",
    "\n",
    "\n",
    "\n",
    "        # >>>>> TAMPILKAN TABEL <<<<<\n",
    "\n",
    "        columns = ['Skenario', 'Metode', 'Akurasi (Top-1)', 'Recall', 'F1-Score', 'Peningkatan']\n",
    "\n",
    "\n",
    "\n",
    "        for model in models:\n",
    "\n",
    "            if all_tables[model]:\n",
    "\n",
    "                table_df = pd.DataFrame(all_tables[model], columns=columns)\n",
    "\n",
    "                print(f\"\\n{'='*120}\")\n",
    "\n",
    "                print(f\"Tabel Perbandingan Kinerja - {model_names[model]}\".center(120))\n",
    "\n",
    "                print(f\"{'='*120}\")\n",
    "\n",
    "                print(table_df.to_string(index=False))\n",
    "\n",
    "                print(f\"{'='*120}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                # Save to CSV\n",
    "\n",
    "                try:\n",
    "\n",
    "                    csv_path = os.path.join(RESULTS_PATH, f'comparison_table_{model}_v6.4.3.csv')\n",
    "\n",
    "                    table_df.to_csv(csv_path, index=False)\n",
    "\n",
    "                    print(f\"Tabel {model_names[model]} disimpan ke: {csv_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f\"Gagal menyimpan tabel {model}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "        # >>>>> BAGIAN 3: SUMMARY STATISTIK <<<<<\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "\n",
    "        print(\"BAGIAN 3: SUMMARY PENINGKATAN KINERJA\".center(80))\n",
    "\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate overall improvement\n",
    "\n",
    "        if not df_original.empty and not df_restored.empty:\n",
    "\n",
    "            # Get common ground truth labels\n",
    "\n",
    "            common_labels = sorted(set(df_original['ground_truth'].unique()) &\n",
    "\n",
    "                                 set(df_restored['ground_truth'].unique()))\n",
    "\n",
    "\n",
    "\n",
    "            if common_labels:\n",
    "\n",
    "                overall_orig = calculate_metrics(df_original, '_original', common_labels)\n",
    "\n",
    "                overall_rest = calculate_metrics(df_restored, '_restored', common_labels)\n",
    "\n",
    "\n",
    "\n",
    "                summary_data = []\n",
    "\n",
    "                for model in models:\n",
    "\n",
    "                    if model in overall_orig and model in overall_rest:\n",
    "\n",
    "                        acc_orig = overall_orig[model]['accuracy']\n",
    "\n",
    "                        acc_rest = overall_rest[model]['accuracy']\n",
    "\n",
    "                        improvement = ((acc_rest - acc_orig) / acc_orig * 100) if acc_orig > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "                        summary_data.append([\n",
    "\n",
    "                            model_names[model],\n",
    "\n",
    "                            f'{acc_orig:.2%}',\n",
    "\n",
    "                            f'{acc_rest:.2%}',\n",
    "\n",
    "                            f'{improvement:+.1f}%'\n",
    "\n",
    "                        ])\n",
    "\n",
    "\n",
    "\n",
    "                if summary_data:\n",
    "\n",
    "                    summary_df = pd.DataFrame(summary_data,\n",
    "\n",
    "                                            columns=['Model', 'Akurasi Original', 'Akurasi Restored', 'Peningkatan'])\n",
    "\n",
    "                    print(summary_df.to_string(index=False))\n",
    "\n",
    "                    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                    # Save summary\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        summary_path = os.path.join(RESULTS_PATH, 'summary_improvement_v3.csv')\n",
    "\n",
    "                        summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "                        print(f\"Summary peningkatan disimpan ke: {summary_path}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "\n",
    "                        print(f\"Gagal menyimpan summary: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"\\*80)\n",
    "\n",
    "print(\"ANALISIS SELESAI\".center(80))\n",
    "\n",
    "print(\"=\"\\*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
