{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Rekognisi Wajah v2: Perbaikan Efisiensi dan Evaluasi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 1: Instalasi Pustaka yang Diperlukan\n",
        "\n",
        "Sel ini memastikan semua pustaka yang dibutuhkan terinstal, termasuk `pybrisque` dan `pyiqa` untuk analisis kualitas gambar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install gfpgan deepface facexlib tqdm pandas matplotlib scikit-learn seaborn pybrisque pyiqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 2: Impor Pustaka & Konfigurasi Path\n",
        "\n",
        "Mengimpor semua modul yang diperlukan dan mendefinisikan path utama untuk data, hasil, dan cache. Menggunakan `sys.executable` untuk memastikan portabilitas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from deepface import DeepFace\n",
        "from gfpgan import GFPGANer\n",
        "from brisque import BRISQUE\n",
        "import pyiqa\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Konfigurasi Path --- \n",
        "BASE_DIR = os.path.abspath('.')\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, '..'))\n",
        "GALLERY_PATH = os.path.join(BASE_DIR, 'data', 'gallery')\n",
        "PROBES_PATH = os.path.join(BASE_DIR, 'data', 'probes')\n",
        "RESULTS_PATH = os.path.join(BASE_DIR, 'results_v2') # Folder hasil baru\n",
        "CACHE_PATH = os.path.join(BASE_DIR, 'cache') # Folder cache baru\n",
        "\n",
        "# Pastikan folder results dan cache ada\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "os.makedirs(CACHE_PATH, exist_ok=True)\n",
        "\n",
        "VENV_PYTHON_PATH = sys.executable # Path python yang robust\n",
        "EMBEDDING_CACHE_FILE = os.path.join(CACHE_PATH, 'embedding_cache.pkl')\n",
        "\n",
        "print(f\"Notebook berjalan di: {BASE_DIR}\")\n",
        "print(f\"Galeri Referensi: {GALLERY_PATH}\")\n",
        "print(f\"Citra Uji (Probes): {PROBES_PATH}\")\n",
        "print(f\"Hasil akan disimpan di: {RESULTS_PATH}\")\n",
        "print(f\"Cache akan disimpan di: {CACHE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 3: Inisialisasi Model-Model Utama\n",
        "\n",
        "Semua model (GFPGAN, DeepFace, BRISQUE, NIQE) diinisialisasi satu kali di sini untuk efisiensi maksimal. Ini menghindari pemuatan ulang model di dalam loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Menggunakan device: {DEVICE}')\n",
        "\n",
        "# Inisialisasi GFPGAN Restorer\n",
        "print(\"Memuat model GFPGAN v1.4...\")\n",
        "gfpgan_restorer = GFPGANer(\n",
        "    model_path=os.path.join(PROJECT_ROOT, 'model_gfpgan', 'gfpgan', 'weights', 'GFPGANv1.4.pth'),\n",
        "    upscale=2,\n",
        "    arch='clean',\n",
        "    channel_multiplier=2,\n",
        "    bg_upsampler=None,\n",
        "    device=DEVICE\n",
        " )\n",
        "print(\"Model GFPGAN siap.\")\n",
        "\n",
        "# Inisialisasi BRISQUE Quality Assessor\n",
        "print(\"Memuat model BRISQUE...\")\n",
        "brisque_assessor = BRISQUE() # url=False untuk path lokal\n",
        "print(\"Model BRISQUE siap.\")\n",
        "\n",
        "# Inisialisasi NIQE Quality Assessor\n",
        "print(\"Memuat model NIQE...\")\n",
        "niqe_assessor = pyiqa.create_metric('niqe', device=DEVICE)\n",
        "print(\"Model NIQE siap.\")\n",
        "\n",
        "# Pre-load model DeepFace untuk 'warming up'\n",
        "print(\"Warm-up model DeepFace (ArcFace)...\")\n",
        "try:\n",
        "    _ = DeepFace.represent(np.zeros((112, 112, 3)), model_name='ArcFace', enforce_detection=False) \n",
        "    print(\"Model DeepFace siap.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal warm-up DeepFace: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 4: Fungsi-Fungsi Utilitas dan Pipeline\n",
        "\n",
        "Fungsi-fungsi bantuan didefinisikan di sini. `get_embedding` sekarang memiliki mekanisme caching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_filename(filename):\n",
        "    try:\n",
        "        base_name = os.path.basename(filename)\n",
        "        parts = os.path.splitext(base_name)[0].split('_')\n",
        "        if len(parts) < 5: return None\n",
        "\n",
        "        subject_id, height_id, distance_id = parts[0], parts[2], parts[4]\n",
        "        if height_id == 'na' or distance_id == 'na': return None\n",
        "\n",
        "        distance = 17 - (int(distance_id) / 2)\n",
        "        if int(distance_id) > 24: distance_category = 'dekat'\n",
        "        elif 14 <= int(distance_id) <= 24: distance_category = 'menengah'\n",
        "        else: distance_category = 'jauh'\n",
        "\n",
        "        height_map = {'3': 'rendah', '5': 'tinggi'}\n",
        "        height_category = height_map.get(height_id)\n",
        "        if not height_category: return None\n",
        "\n",
        "        return {\n",
        "            'subject_id': subject_id,\n",
        "            'distance_m': distance,\n",
        "            'distance_category': distance_category,\n",
        "            'height_id': height_id,\n",
        "            'height_category': height_category\n",
        "        }\n",
        "    except (IndexError, ValueError):\n",
        "        return None\n",
        "\n",
        "def get_embedding(image_path_or_array, model_name='ArcFace', detector_backend='retinaface'):\n",
        "    try:\n",
        "        embedding_obj = DeepFace.represent(\n",
        "            img_path=image_path_or_array,\n",
        "            model_name=model_name,\n",
        "            enforce_detection=False,\n",
        "            detector_backend=detector_backend\n",
        "        )\n",
        "        return embedding_obj[0]['embedding']\n",
        "    except (ValueError, AttributeError, IndexError, TypeError):\n",
        "        return None\n",
        "\n",
        "def find_best_match(probe_embedding, gallery_embeddings):\n",
        "    if probe_embedding is None or not gallery_embeddings:\n",
        "        return None, float('inf')\n",
        "    min_dist = float('inf')\n",
        "    best_match_id = None\n",
        "    for subject_id, gallery_embedding in gallery_embeddings.items():\n",
        "        if gallery_embedding is None: continue\n",
        "        dist = cosine(probe_embedding, gallery_embedding)\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            best_match_id = subject_id\n",
        "    return best_match_id, min_dist\n",
        "\n",
        "def get_niqe_score(image_array, assessor):\n",
        "    try:\n",
        "        # Konversi BGR (OpenCV) ke RGB, lalu ke Tensor [0,1] C,H,W\n",
        "        img_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
        "        img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
        "        score = assessor(img_tensor.to(DEVICE)).item()\n",
        "        return score\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "print(\"Fungsi utilitas siap digunakan.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 5: Pembuatan Galeri Referensi\n",
        "\n",
        "Membuat database fitur dari gambar referensi berkualitas tinggi. Hasilnya disimpan dalam memori.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gallery_embeddings = {}\n",
        "gallery_files = glob.glob(os.path.join(GALLERY_PATH, '*.jpg'))\n",
        "\n",
        "print(f\"Membuat database fitur dari {len(gallery_files)} gambar di galeri...\")\n",
        "for g_file in tqdm(gallery_files):\n",
        "    subject_id = os.path.basename(g_file).split('_')[0]\n",
        "    embedding = get_embedding(g_file)\n",
        "    if embedding is not None:\n",
        "        gallery_embeddings[subject_id] = embedding\n",
        "\n",
        "print(f\"Database fitur galeri berhasil dibuat untuk subjek: {list(gallery_embeddings.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 6: Eksekusi Pipeline Utama (Versi 2)\n",
        "\n",
        "Ini adalah inti dari eksperimen. Loop ini akan memproses semua gambar uji, menjalankan kedua jalur, menghitung metrik kualitas, dan menyimpan semua hasil secara terstruktur.\n",
        "Kegagalan pada tahap manapun (restorasi, ekstraksi fitur) akan dicatat dan dianggap sebagai prediksi yang salah untuk evaluasi yang adil.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muat cache jika ada\n",
        "try:\n",
        "    with open(EMBEDDING_CACHE_FILE, 'rb') as f:\n",
        "        embedding_cache = pickle.load(f)\n",
        "    print(f\"Berhasil memuat {len(embedding_cache)} embedding dari cache.\")\n",
        "except FileNotFoundError:\n",
        "    embedding_cache = {}\n",
        "    print(\"Cache embedding tidak ditemukan, akan membuat baru.\")\n",
        "probe_files = glob.glob(os.path.join(PROBES_PATH, '*.JPG'))\n",
        "results_v2 = []\n",
        "print(f\"Memulai pemrosesan {len(probe_files)} citra uji...\")\n",
        "start_time = time.time()\n",
        "for probe_path in tqdm(probe_files):\n",
        "    metadata = parse_filename(probe_path)\n",
        "    if not metadata:\n",
        "        continue\n",
        "    probe_filename = os.path.basename(probe_path)\n",
        "    ground_truth = metadata['subject_id']\n",
        "    # --- Inisialisasi hasil untuk iterasi ini ---\n",
        "    prediction_A, prediction_B = None, None\n",
        "    is_correct_A, is_correct_B = False, False\n",
        "    restoration_succeeded = False\n",
        "    brisque_original, brisque_restored = None, None\n",
        "    niqe_original, niqe_restored = None, None\n",
        "    # --- Jalur A (Tanpa Restorasi) ---\n",
        "    if probe_filename in embedding_cache:\n",
        "        embedding_A = embedding_cache[probe_filename]\n",
        "    else:\n",
        "        embedding_A = get_embedding(probe_path)\n",
        "        embedding_cache[probe_filename] = embedding_A\n",
        "    if embedding_A is not None:\n",
        "        prediction_A, _ = find_best_match(embedding_A, gallery_embeddings)\n",
        "        if prediction_A == ground_truth:\n",
        "            is_correct_A = True\n",
        "    # --- Jalur B (Dengan Restorasi) ---\n",
        "    try:\n",
        "        img_original = cv2.imread(probe_path, cv2.IMREAD_COLOR)\n",
        "        if img_original is not None:\n",
        "            # Hitung metrik kualitas untuk gambar asli\n",
        "            brisque_original = brisque_assessor.score(img_original)\n",
        "            niqe_original = get_niqe_score(img_original, niqe_assessor)\n",
        "            # Lakukan restorasi\n",
        "            _, _, restored_img = gfpgan_restorer.enhance(img_original, has_aligned=False,\n",
        "only_center_face=True, paste_to_face=True)\n",
        "            if restored_img is not None:\n",
        "                restoration_succeeded = True\n",
        "                # Hitung metrik kualitas untuk gambar hasil restorasi\n",
        "                brisque_restored = brisque_assessor.score(restored_img)\n",
        "                niqe_restored = get_niqe_score(restored_img, niqe_assessor)\n",
        "                # Dapatkan embedding dari gambar hasil restorasi\n",
        "                embedding_B = get_embedding(restored_img)\n",
        "                if embedding_B is not None:\n",
        "                    prediction_B, _ = find_best_match(embedding_B, gallery_embeddings)\n",
        "                    if prediction_B == ground_truth:\n",
        "                        is_correct_B = True\n",
        "    except Exception as e:\n",
        "        # Jika ada error di tahap manapun di Jalur B, hasilnya dianggap gagal\n",
        "        print(f\"Error processing {probe_filename}: {e}\") # INI BAGIAN PENTING UNTUK DEBUGGING\n",
        "        is_correct_B = False\n",
        "        restoration_succeeded = False\n",
        "    results_v2.append({\n",
        "        'file': probe_filename,\n",
        "        'metadata': metadata,\n",
        "        'ground_truth': ground_truth,\n",
        "        'prediction_A': prediction_A,\n",
        "        'is_correct_A': is_correct_A,\n",
        "        'prediction_B': prediction_B,\n",
        "        'is_correct_B': is_correct_B,\n",
        "        'restoration_succeeded': restoration_succeeded,\n",
        "        'brisque_original': brisque_original,\n",
        "        'brisque_restored': brisque_restored,\n",
        "        'niqe_original': niqe_original,\n",
        "        'niqe_restored': niqe_restored\n",
        "    })\n",
        "# --- Simpan Hasil dan Cache ---\n",
        "end_time = time.time()\n",
        "print(f\"Selesai memproses {len(results_v2)} citra uji dalam {end_time - start_time:.2f} detik.\")\n",
        "results_file_path = os.path.join(RESULTS_PATH, 'pipeline_results_v2.json')\n",
        "with open(results_file_path, 'w') as f:\n",
        "    json.dump(results_v2, f, indent=4)\n",
        "print(f\"Variabel 'results_v2' berhasil disimpan ke: {results_file_path}\")\n",
        "with open(EMBEDDING_CACHE_FILE, 'wb') as f:\n",
        "    pickle.dump(embedding_cache, f)\n",
        "print(f\"Cache embedding berhasil disimpan ke: {EMBEDDING_CACHE_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Langkah 7: Analisis Hasil dan Visualisasi\n",
        "\n",
        "Bagian ini memuat kembali hasil yang baru saja disimpan, lalu menghitung metrik evaluasi dan membuat visualisasi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muat hasil dari file JSON untuk analisis\n",
        "results_file_path = os.path.join(RESULTS_PATH, 'pipeline_results_v2.json')\n",
        "with open(results_file_path, 'r') as f:\n",
        "    results_data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(results_data)\n",
        "# Konversi metadata dictionary menjadi kolom terpisah\n",
        "df_meta = pd.json_normalize(df['metadata'])\n",
        "df = pd.concat([df.drop('metadata', axis=1), df_meta], axis=1)\n",
        "\n",
        "print(f\"Data hasil berhasil dimuat. Total {len(df)} baris.\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.1 Analisis Performa Keseluruhan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_summary_table(df, group_by_col):\n",
        "    summary_list = []\n",
        "    categories = df[group_by_col].unique()\n",
        "\n",
        "    for category in categories:\n",
        "        subset_df = df[df[group_by_col] == category]\n",
        "        count = len(subset_df)\n",
        "        \n",
        "        # Metrik Jalur A\n",
        "        accuracy_A = subset_df['is_correct_A'].sum() / count if count > 0 else 0\n",
        "        \n",
        "        # Metrik Jalur B\n",
        "        accuracy_B = subset_df['is_correct_B'].sum() / count if count > 0 else 0\n",
        "        \n",
        "        # Peningkatan\n",
        "        improvement = ((accuracy_B - accuracy_A) / accuracy_A * 100) if accuracy_A > 0 else float('inf')\n",
        "        \n",
        "        summary_list.append({\n",
        "            'Kategori': category,\n",
        "            'Jumlah Sampel': count,\n",
        "            'Akurasi (Jalur A)': f'{accuracy_A:.2%}',\n",
        "            'Akurasi (Jalur B)': f'{accuracy_B:.2%}',\n",
        "            'Peningkatan Akurasi': f'{improvement:+.2f}%' if improvement != float('inf') else 'N/A'\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(summary_list).set_index('Kategori')\n",
        "\n",
        "# Analisis berdasarkan Jarak\n",
        "summary_distance = create_summary_table(df, 'distance_category')\n",
        "print(\"--- Analisis Berdasarkan Jarak ---\")\n",
        "display(summary_distance.reindex(['dekat', 'menengah', 'jauh']))\n",
        "\n",
        "# Analisis berdasarkan Ketinggian\n",
        "summary_height = create_summary_table(df, 'height_category')\n",
        "print(\"\\n--- Analisis Berdasarkan Ketinggian ---\")\n",
        "display(summary_height.reindex(['rendah', 'tinggi']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.2 Analisis Kualitas Gambar (BRISQUE & NIQE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iqa_df = df[['brisque_original', 'brisque_restored', 'niqe_original', 'niqe_restored']].copy()\n",
        "iqa_df.dropna(inplace=True) # Hanya analisis gambar yang berhasil direstorasi\n",
        "\n",
        "avg_brisque_original = iqa_df['brisque_original'].mean()\n",
        "avg_brisque_restored = iqa_df['brisque_restored'].mean()\n",
        "avg_niqe_original = iqa_df['niqe_original'].mean()\n",
        "avg_niqe_restored = iqa_df['niqe_restored'].mean()\n",
        "\n",
        "print(\"--- Analisis Kualitas Gambar (BRISQUE & NIQE) ---\")\n",
        "print(f\"Skor rata-rata BRISQUE (Asli): {avg_brisque_original:.2f} (Lebih tinggi = kualitas lebih rendah)\")\n",
        "print(f\"Skor rata-rata BRISQUE (Restorasi): {avg_brisque_restored:.2f} (Lebih rendah = kualitas lebih baik)\")\n",
        "print('---')\n",
        "print(f\"Skor rata-rata NIQE (Asli): {avg_niqe_original:.2f} (Lebih tinggi = kualitas lebih rendah)\")\n",
        "print(f\"Skor rata-rata NIQE (Restorasi): {avg_niqe_restored:.2f} (Lebih rendah = kualitas lebih baik)\")\n",
        "\n",
        "# Visualisasi dengan Box Plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(data=iqa_df, palette=['skyblue', 'lightgreen', 'salmon', 'lightcoral'])\n",
        "plt.title('Perbandingan Skor Kualitas Gambar Sebelum dan Sesudah Restorasi')\n",
        "plt.ylabel('Skor IQA')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "iqa_chart_path = os.path.join(RESULTS_PATH, 'iqa_comparison_v2.png')\n",
        "plt.savefig(iqa_chart_path)\n",
        "plt.show()\n",
        "print(f\"Grafik IQA disimpan di {iqa_chart_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.3 Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_v2(df, pipeline_type, labels):\n",
        "    if pipeline_type == 'A':\n",
        "        title = 'Confusion Matrix - Jalur A (Tanpa Restorasi)'\n",
        "        pred_key = 'prediction_A'\n",
        "    else:\n",
        "        title = 'Confusion Matrix - Jalur B (Dengan Restorasi)'\n",
        "        pred_key = 'prediction_B'\n",
        "    \n",
        "    # Filter data yang valid untuk matriks\n",
        "    cm_df = df.dropna(subset=['ground_truth', pred_key])\n",
        "    y_true = cm_df['ground_truth']\n",
        "    y_pred = cm_df[pred_key]\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, linewidths=.5)\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.ylabel('Label Sebenarnya (True Label)')\n",
        "    plt.xlabel('Label Prediksi (Predicted Label)')\n",
        "    \n",
        "    file_path = os.path.join(RESULTS_PATH, f'confusion_matrix_{pipeline_type}_v2.png')\n",
        "    plt.savefig(file_path, bbox_inches='tight')\n",
        "    print(f\"Confusion Matrix untuk Jalur {pipeline_type} disimpan di: {file_path}\")\n",
        "    plt.show()\n",
        "\n",
        "unique_labels = sorted(df['ground_truth'].unique())\n",
        "plot_confusion_matrix_v2(df, 'A', unique_labels)\n",
        "plot_confusion_matrix_v2(df, 'B', unique_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.4 Laporan Klasifikasi per Subjek\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Laporan Klasifikasi Jalur A (Tanpa Restorasi) ---\")\n",
        "report_A_df = df.dropna(subset=['ground_truth', 'prediction_A'])\n",
        "print(classification_report(report_A_df['ground_truth'], report_A_df['prediction_A'], labels=unique_labels))\n",
        "\n",
        "print(\"\\n--- Laporan Klasifikasi Jalur B (Dengan Restorasi) ---\")\n",
        "report_B_df = df.dropna(subset=['ground_truth', 'prediction_B'])\n",
        "print(classification_report(report_B_df['ground_truth'], report_B_df['prediction_B'], labels=unique_labels))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
